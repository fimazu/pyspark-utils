{"cells":[{"cell_type":"markdown","source":["#Utils: General Shared Functions\n** Description: ** Notebook created to deliver some common use Functions in Databricks to make our lives easier.\n___\n> ** FUNCTION: createTableFromDF(dfUtil,database,tableName,area,subject,dropTable,writeMode): ** <BR>\nUsed to Create a Delta Table from a Spark Dataframe in one of the existing Databases in a dynamic way.\nThe Table Schema has to be already defined in the parameter DataFrame so the Table's Columns are created with the appropriate data formats.\n>>** Parameters Needed: **\n * **dfUtil:** (Spark DF) The Dataframe containing the data that you want to create the Delta Table.\n * **database:** (String) use the 'sandbox' value to include the table in the sandbox database.\n * **tableName:** (String) the name of the Table you want to create in Databricks.\n * **area:** (String) the name of your area in the Sandbox (egs: 'credit-risk','data-engineers','data-scientists').\n * **subject:** (String) the subject/project/username that you are creating this table for. Can be you username in the Sandbox\n * **dropTable:** (Boolean) if you want to Drop the table from the database before creating it. (eg: True/False)\n * **writeMode:** (String) The write mode for you to record the data in the table. (eg: 'append': always appends the data in the table / 'overwrite': always overwrites the data in the table.)\n  \n>>** Example Code: ** <BR>\n createTableFromDF( dftst , 'sandbox' , 'test_table_name' , 'data-engineers' , 'fimazu' , False , 'append' ) <BR>\n\n> ** <br> FUNCTION: moveFileFromDBFSToSandbox(area,sourceFile,targetFile): ** <BR>\nUsed to move files uploaded manually into Databricks DBFS to the target area Sandbox. <BR>\n  \n>>** Parameters Needed: **\n * **area:** (String) the name of your area in the Sandbox (egs: 'credit-risk','data-engineers','data-scientists').\n * **sourceFile:** (String) the name of the source file uploaded into DBFS.\n * **targetFile:** (String) the name of target file to be copied to the area Sandbox. the file can be renamed.\n  \n>>** Example Code: ** <BR>\n moveFileFromDBFSToSandbox('data-engineers','testSourceFile.csv','testDestinationFile.csv'):\n\n> ** <br> FUNCTION: flatten_structs(nested_df): ** <BR>\nUsed to flatten nested structured columns in a Dataframe (eg: Struct type columns). Just pass the dataframe containing the Nested Columns as parameter. <BR>\n  \n>>** Parameters Needed: **\n * **Nested Data Frame (df):** The Dataframe containing the desired columns that you need un-nested. A new Dataframe will be returned.\n  \n>>** Example Code: ** <BR>\n df_Final = flatten_structs( df_StructColumns )  \n  \n> ** <br> FUNCTION: flatten_array_struct_df(nested_df): ** <BR>\nUsed to flatten nested columns in a Dataframe (eg: Array type columns). Just pass the dataframe containing the Nested Columns as parameter. <BR>\n  \n>>** Parameters Needed: **\n * **Nested Data Frame (df):** The Dataframe containing the desired columns that you want need un-nested. A new Dataframe will be returned.\n  \n>>** Example Code: ** <BR>\n df_Final = flatten_array_struct_df( df_NestedColumns )\n\n> ** <br> FUNCTION: parseJSONColumns(nested_df): ** <BR>\nUsed to deserialize Json Columns that are in string format and flattens these nested json columns in a Dataframe (eg: Struct type columns). \nJust pass the dataframe containing the Nested Columns as parameter. <BR>\n  \n>>** Parameters Needed: **\n * **Serialized Json Data Frame (df):** The Dataframe containing the desired columns that you need deserialized and un-nested. A new Dataframe will be returned.\n  \n>>** Example Code: ** <BR>\n df_Final = parseJSONColumns( df_SerializedColumns )"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"c64bedb4-a525-4a18-a1f2-dcfbdfec6909"}}},{"cell_type":"code","source":["#general configurations and imports for the notebook run\n#SET spark.databricks.delta.schema.autoMerge.enabled=True\nspark.conf.set(\"spark.sql.sources.partitionOverwriteMode\",\"dynamic\")\nspark.conf.set(\"spark.sql.crossJoin.enabled\", \"true\")\nspark.sql(\"set spark.sql.legacy.timeParserPolicy=LEGACY\")\nsqlContext.setConf(\"spark.sql.shuffle.partitions\", \"20\")\n\nfrom pyspark.sql.types import *\nimport os, shutil, glob, re, json, requests, time\nfrom pyspark.sql.functions import * #col, lit, udf, substring\nfrom pyspark.sql.window import Window\nfrom datetime import datetime, date, timedelta\nfrom pyspark.sql import functions as f"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"8945b4bc-8638-4767-ae6f-efdde52bae63"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\"></div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":0},{"cell_type":"code","source":["# FUNCTION: createTableFromDF(dfUtil,database,tableName,area,subject,dropTable,writeMode):\n#   Used to Create a Delta Table from a Spark Dataframe in one of the existing Databases in a dynamic way. The Table Schema has to be already defined in the parameter DataFrame so the Table's Columns are created with the appropriate data formats.\n#\n# Parameters Needed:\n#  - dfUtil: (Spark DF) The Dataframe containing the data that you want to create the Delta Table.\n#  - database: (String) use the 'sandbox' value to include the table in the sandbox database.\n#  - tableName: (String) the name of the Table you want to create in Databricks.\n#  - area: (String) the name of your area in the Sandbox (egs: 'credit-risk','data-engineers','data-scientists').\n#  - subject: (String) the subject/project/username that you are creating this table for. Can be you username in the Sandbox\n#  - dropTable: (Boolean) if you want to Drop the table from the database before creating it. (eg: True/False)\n#  - writeMode: (String) The write mode for you to record the data in the table. (eg: 'append': always appends the data in the table / 'overwrite': always overwrites the data in the table.)\n#\n# Example Code:\n#   createTableFromDF( dftst , 'sandbox' , 'test_table_name' , 'data-engineers' , 'fimazu' , False , 'append' )\ndef createTableFromDF(dfUtil,database,tableName,area,subject,dropTable,writeMode):\n  #writeMode = 'append'/'overwrite'\n  mntUtil = '/mnt/'+database+'/'+area+'/'+subject+'/'+tableName\n  if dropTable:    \n    dbutils.fs.rm(mntUtil,True)\n    spark.sql(\"\"\"DROP TABLE IF EXISTS \"\"\"+database+\"\"\".\"\"\"+tableName)\n  \n  #Create the Curated table for Kustomer Conversations\n  dfUtil.write.format('delta').option('mergeSchema', 'true').mode(writeMode).save(mntUtil)\n  #Creating the Raw table in the DB if not exists\n  spark.sql(\"\"\"\n      CREATE TABLE IF NOT EXISTS \"\"\"+database+\"\"\".\"\"\"+tableName+\"\"\" \n      USING DELTA LOCATION '/mnt/\"\"\"+database+\"\"\"/\"\"\"+area+\"\"\"/\"\"\"+subject+\"\"\"/\"\"\"+tableName+\"\"\"'\n      \"\"\")"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"f2a91ec0-238e-4d29-a92d-baf9829bf8fb"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\"></div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":0},{"cell_type":"code","source":["# FUNCTION: moveFileFromDBFSToSandbox(area,sourceFile,targetFile):\n#   Used to move files uploaded manually into Databricks DBFS to the target area Sandbox.\n# Parameters Needed:\n#  - area: (String) the name of your area in the Sandbox (egs: 'credit-risk','data-engineers','data-scientists').\n#  - sourceFile: (String) the name of the source file uploaded into DBFS.\n#  - targetFile: (String) the name of target file to be copied to the area Sandbox. the file can be renamed.\n# Example Code:\n#   moveFileFromDBFSToSandbox('data-engineers','testSourceFile.csv','testDestinationFile.csv'):\ndef moveFileFromDBFSToSandbox(area,sourceFile,targetFile):\n  mntFrom = 'dbfs:/FileStore/tables/' + area + '/' + sourceFile\n  mntTo   = '/mnt/sandbox/' + area + '/' + targetFile\n  #copy/move the file to a sandbox folder or other \"mounted\" folder in Databricks\n  dbutils.fs.mv(mntFrom,mntTo)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"e3914002-a800-44e9-bdb0-33e711326028"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\"></div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":0},{"cell_type":"code","source":["# FUNCTION: flatten_structs(nested_df):\n#   Used to flatten nested structured columns in a Dataframe (eg: Struct type columns). Just pass the dataframe containing the Nested Columns as parameter.\n# Parameters Needed:\n#  - Nested Data Frame (df): The Dataframe containing the desired columns that you need un-nested. A new Dataframe will be returned.\n# Example Code:\n#   df_Final = flatten_array_struct_df( df_NestedColumns )\ndef flatten_structs(nested_df):\n    stack = [((), nested_df)]\n    columns = []\n\n    while len(stack) > 0:\n        \n        parents, df = stack.pop()\n        \n        array_cols = [\n            c[0]\n            for c in df.dtypes\n            if c[1][:5] == \"array\"\n        ]\n        \n        flat_cols = [\n            f.col(\".\".join(parents + (c[0],))).alias(\"_\".join(parents + (c[0],)))\n            for c in df.dtypes\n            if c[1][:6] != \"struct\"\n        ]\n\n        nested_cols = [\n            c[0]\n            for c in df.dtypes\n            if c[1][:6] == \"struct\"\n        ]\n        \n        columns.extend(flat_cols)\n\n        for nested_col in nested_cols:\n            projected_df = df.select(nested_col + \".*\")\n            stack.append((parents + (nested_col,), projected_df))\n        \n    return nested_df.select(columns)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"bebc2c9a-5157-4ca9-b5c2-f002f1786d88"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\">Out[2]: &lt;function __main__.flatten_array_struct_df(df)&gt;</div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">Out[2]: &lt;function __main__.flatten_array_struct_df(df)&gt;</div>"]}}],"execution_count":0},{"cell_type":"code","source":["# FUNCTION: flatten_array_struct_df(nested_df):\n#   Used to flatten nested columns in a Dataframe (eg: Struct or Array type columns). Just pass the dataframe containing the Nested Columns as parameter.\n# Parameters Needed:\n#  - Nested Data Frame (df): The Dataframe containing the desired columns that you want need un-nested. A new Dataframe will be returned.\n# Example Code:\n#   df_Final = flatten_array_struct_df( df_NestedColumns )\ndef flatten_array_struct_df(df):\n    \n    array_cols = [\n            c[0]\n            for c in df.dtypes\n            if c[1][:5] == \"array\"\n        ]\n    \n    while len(array_cols) > 0:\n        \n        for array_col in array_cols:\n            \n            cols_to_select = [x for x in df.columns if x != array_col ]\n            \n            df = df.withColumn(array_col, f.explode(f.col(array_col)))\n            \n        df = flatten_structs(df)\n        \n        array_cols = [\n            c[0]\n            for c in df.dtypes\n            if c[1][:5] == \"array\"\n        ]\n    return df\nspark.udf.register(\"flatten_array_struct_df\", flatten_array_struct_df)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"86475c9f-697b-4960-8032-b9607eef1d32"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"data":"","errorSummary":"","metadata":{},"errorTraceType":null,"type":"ipynbError","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>"]}}],"execution_count":0},{"cell_type":"code","source":["#Function that checks just if a String is a serialized Json string. Returns Boolean(True/False)\ndef is_json(myjson):\n  try:\n    json_object = json.loads(myjson)\n  except ValueError as e:\n    return False\n  return True\n\n# FUNCTION: parseJSONColumns(nested_df):\n#   Used to deserialize Json Columns that are in string format and flattens these nested json columns in a Dataframe (eg: Struct type columns). Just pass the dataframe containing the Nested Columns as parameter.\n# Parameters Needed:\n#  - Serialized Json Data Frame (df): The Dataframe containing the desired columns that you need deserialized and un-nested. A new Dataframe will be returned.\n#  - flatten_struct_cols (True/False): Parameter to set call for the \"flatten_structs\" Function to transform Json attributes into actual DF Columns\n# Example Code:\n#   df_Final = parseJSONColumns( df_SerializedColumns )\ndef parseJSONColumns(df, flatten_struct_cols, sanitize=True):\n    #iterates through all DF columns\n    for i in df.columns:\n        #gets String value from each specific Column to validate if it's a de-serializable Json\n        test_json = str(df.distinct().rdd.flatMap(lambda x: x).collect()[df.columns.index(i)])\n        #if Column content is a Json, procedes to the next steps\n        if is_json(test_json):\n          # sanitize if requested.\n          if sanitize:\n              df = (\n                  df.withColumn(\n                      i,\n                      concat(lit('{\"data\": '), i, lit('}'))\n                  )\n              )\n          # infer schema and apply it\n          schema = spark.read.json(df.rdd.map(lambda x: x[i])).schema\n          df = df.withColumn(i, from_json(col(i), schema))\n\n          # unpack the wrapped object if needed\n          if sanitize:\n              df = df.withColumn(i, col(i).data)\n    #Calls the flatten_structs Function to transform Json attributes into DF Columns if parameter is True\n    if flatten_struct_cols:\n      df = flatten_structs(df)\n    return df"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"c58a050a-03b7-446d-a0c0-29c2ee64ca42"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\"></div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":0}],"metadata":{"application/vnd.databricks.v1+notebook":{"notebookName":"utils","dashboards":[],"notebookMetadata":{"pythonIndentUnit":2},"language":"python","widgets":{},"notebookOrigID":3670656612409926}},"nbformat":4,"nbformat_minor":0}
